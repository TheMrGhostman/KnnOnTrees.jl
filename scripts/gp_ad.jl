#using Revise
using ArgParse, DrWatson, BSON, DataFrames, Random, Serialization
using Flux, Zygote, Mill, Statistics, KnnOnTrees, LinearAlgebra
using Wandb, Dates, Logging, ProgressBars, BenchmarkTools, StatsBase
using HMillDistance, Optim, Base.Threads, EvalMetrics

using AbstractGPs, Distributions, KernelFunctions


s = ArgParseSettings()
@add_arg_table! s begin
    "dataset"
        arg_type = String
        help = "Name of dataset to use"
        default = "Mutagenesis"
    "seed"
        arg_type = Int
        help = "Random seed for initialization of data splits"
        default = 666
    "iters"
        arg_type = Int
        help = "Number or iterations"
        default = 10
    "learning_rate"        
        arg_type = Float64
        help = "Learning rate for optimizer"
        default = 0.01
    "kernel"        
        arg_type = String
        help = "Select Kernel option: (\"Laplacian\", \"Gaussian\", \"Matern32\") "
        default = "Laplacian"
    "gamma"
        arg_type = String
        help = "let Î³ parameter trainable \"trainable\" or \"nontrainable\" "
        default = "nontrainable"
    "ui"
        arg_type = Int
        help = "unique identifier"
        default = Int(rand(1:1e8))
    "homogen_depth"
        arg_type = Int
        help = "Depth of tree which is created from homogenous Graphs (TUDataset). For other data argument is ignored!"
        default = 4
    "bag_metric"
        arg_type = String
        help = "Which type of metric to use for comparision of bags \\
                 options: (\"ChamferDistance\", \"WassersteinProbDist\", \"WassersteinMultiset\", \"Hausdorff\" )"
        default="ChamferDistance"
    "card_metric"
        arg_type = String
        help = "Which type of metric/transfromation to use for cardinality \\
                options: (\"ScaleOne\", \"MaxCard\")"
        default = "ScaleOne" 
end

parsed_args = parse_args(ARGS, s)
@unpack dataset, seed, iters, learning_rate, kernel, gamma, ui, homogen_depth, bag_metric, card_metric = parsed_args
#dataset, seed, iters, kernel, gamma, ui = "Mutagenesis", 666, 1, "Laplacian", "nontrainable",1001
trainable_ = gamma == "trainable";
@info "Threads -> $(nthreads())"

run_name = "GP-AD-$(dataset)-seed=$(seed)-kernel=$(kernel)-gamma=$(gamma)-ui=$(ui)"
# Initialize logger
lg = WandbLogger(project ="KnnOnTrees - Anomaly Detection",
                 name = run_name,
                 config = Dict(
                               "kernel" => kernel,
                               "transformation" => "Softplus",
                               "initialization" => "0.54 â‹… ones",
                               "gamma" => gamma,
                               "dataset" => dataset,
                               "iters" => iters,
                               "learning_rate" =>learning_rate,
                               "homogen_depth" => homogen_depth,
                               "bag_metric" => bag_metric,
                               "card_metric" => card_metric,
                               "seed" => seed,
                               "ui" => ui))

# Use LoggingExtras.jl to log to multiple loggers together
global_logger(lg)


start = time()
data = load_dataset(dataset; to_mill=true, to_pad_leafs=false, depth=homogen_depth);

# separate normal class 
class_freq = countmap(data[2])
most_frequent_class = sort(collect(class_freq), rev=true,by=x->getindex(x, 2))[1][1]

normal_data = data[1][data[2] .== most_frequent_class]; # class is imaginary 0
anomalous_data = data[1][data[2] .!= most_frequent_class]; # every other class is anomalous (leave-one-in procedure)

train_n, val_n, test_n = preprocess(normal_data, zeros(length(normal_data)); ratios=(0.6,0.2,0.2), procedure=:clf, seed=seed, filter_under=10);
_, val_a, test_a = preprocess(anomalous_data, ones(length(anomalous_data)); ratios=(0.0,0.5,0.5), procedure=:clf, seed=seed, filter_under=10);

train = Array.(train_n);
val = map((a,b)->vcat(a,b), val_n, val_a);
test = map((a,b)->vcat(a,b), test_n, test_a);


# bag_metric and card_metric switch
bag_m = getfield(HMillDistance, Symbol(bag_metric))
card_m = getfield(HMillDistance, Symbol(card_metric)) 
# 1) define metrics
metric = reflectmetric(data[1][1], set_metric=bag_m, card_metric=card_m, weight_sampler=x->0.54.*ones(x), weight_transform=softplus)
# 2) specify kernel
KernelConstructor_ = KernelSelector(kernel; trainable=trainable_) 
Kernel = KernelConstructor_(metric)#; Î³=1.0, trainable=trainable_)
# 3) initialize Î¸
Î¸_init, m_st = Flux.destructure(Kernel)
Î¸_names = destructure_metric_to_ws(Kernel.d);

# 4) build_gp function 
function build_gp(Î¸)    
    Î¸_, f_ = Flux.destructure(Kernel)
    kernel_ = f_(Î¸) #KernelConstructor(f_(Î¸)) 
    return GP(kernel_)
end;

loss(Î¸, X) = -logpdf(build_gp(Î¸)(X, 1e-5), ones(size(X, 1)))

# 6) optimise
opt = ADAM(learning_rate)
# 7) training loop
history = Dict("Training/Loss"=>[], "params_stats"=>[])

sqnorm(x,b=0) = sum(y->abs2(y .- b), x)
param_statistics(Î¸) = Dict(
    "minimum" => minimum(Î¸), 
    "q_25" => quantile(Î¸, 0.25), 
    "median" => median(Î¸),
    "q_75" => quantile(Î¸, 0.75),
    "maximum" => maximum(Î¸)
    )

for iter âˆˆ tqdm(1:iters)
    loss_, grads = Zygote.withgradient(Î¸ -> loss(Î¸, train[1]), Î¸_init)
    Flux.Optimise.update!(opt, Î¸_init, grads[1])
    stats = param_statistics(softplus.(Î¸_init))
    Wandb.log(lg, merge(Dict("Training/Loss"=>loss_,), stats));
    push!(history["Training/Loss"], loss_)
    push!(history["params_stats"], stats)
end

# get best/optimized parameters
Î¸_best = deepcopy(Î¸_init)
#Â quickly log parameters
parameters = Wandb.Table(data=hcat(string.(Î¸_names[1]),Î¸_best), columns=["names", "values"])
Wandb.log(lg, Dict("parameters_tab"=>parameters,))

# build gp
fx = build_gp(Î¸_best)(train[1])
# compute posterior
f_post = posterior(fx, ones(size(train[2])))
# compute marginals
Î¼_v, ÏƒÂ²_v = mean_and_var(f_post(val[1]))
Î¼_t, ÏƒÂ²_t = mean_and_var(f_post(test[1]))


#marg_val = marginals(f_post(val[1]))
#marg_test = marginals(f_post(test[1]))
# marginals for val data
#Î¼_v = getproperty.(marg_val, :Î¼)
#Ïƒ_v = getproperty.(marg_val, :Ïƒ)
# marginals for test data
#Î¼_t = getproperty.(marg_test, :Î¼)
#Ïƒ_t = getproperty.(marg_test, :Ïƒ)

ð“”_v = - Î¼_v
ð“”_t = - Î¼_t

ð“¥_v = - ÏƒÂ²_v#Ïƒ_v.^2 # original formula is -Ïƒ^2 but for 1 normal, 0 anomal -> we have 0 normal, 1 anomal
ð“¥_t = - ÏƒÂ²_t#Ïƒ_t.^2


#Â compute AUCs
# score with Î¼
auc_ð“”_val = auc_trapezoidal(prcurve(val[2], ð“”_v)...)
auc_ð“”_test = auc_trapezoidal(prcurve(test[2], ð“”_t)...)
# score with neg var -Ïƒ^2
auc_ð“¥_val = auc_trapezoidal(prcurve(val[2], ð“¥_v)...)
auc_ð“¥_test = auc_trapezoidal(prcurve(test[2], ð“¥_t)...)
# score with Ïƒ
auc_Ïƒ_val = auc_trapezoidal(prcurve(val[2], ÏƒÂ²_v)...)
auc_Ïƒ_test = auc_trapezoidal(prcurve(test[2], ÏƒÂ²_t)...)


Wandb.log(lg, Dict(
    "auc_ð“”_val" => round(auc_ð“”_val, digits=3), 
    "auc_ð“”_test" => round(auc_ð“”_test, digits=3),
    "auc_ð“¥_val" => round(auc_ð“¥_val, digits=3), 
    "auc_ð“¥_test" => round(auc_ð“¥_test, digits=3),
    "auc_Ïƒ_val" => round(auc_Ïƒ_val, digits=3), 
    "auc_Ïƒ_test" => round(auc_Ïƒ_test, digits=3),
    )
);

id = (seed=seed, ui=ui, kernel=kernel)
savedir = datadir("GPs-AD", dataset, "$(seed)")
results = (
    # basic log
    model=build_gp(Î¸_best), # 
    kernel=m_st(Î¸_best), 
    metric=m_st(Î¸_best).d,
    seed=seed, 
    params=Î¸_best, 
    iters=iters, 
    history=history,
    ui=id[:ui],  
    # posterior distributions and fitine GPs
    train_post=f_post, 
    # data splits 
    train=train,
    val=val,
    test=test, 
    #Â predictions
    y_ð“”_valid = ð“”_v,
    y_ð“”_test = ð“”_t,
    y_ð“¥_valid = ð“¥_v,
    y_ð“¥_test = ð“¥_t,
    y_Ïƒ_valid = ÏƒÂ²_v,
    y_Ïƒ_test = ÏƒÂ²_t,
    # metrics
    auc_ð“” = (valid=auc_ð“”_val, test=auc_ð“”_test),
    auc_ð“¥ = (valid=auc_ð“¥_val, test=auc_ð“¥_test),
    auc_Ïƒ = (valid=auc_Ïƒ_val, test=auc_Ïƒ_test),
)

result = Dict{Symbol, Any}([sym=>val for (sym,val) in pairs(results)]); # this has to be a Dict 
if !ispath(savedir)
    mkpath(savedir)
end
serialize(joinpath(savedir, "$(run_name).jls"), result) 
#tagsave(joinpath(savedir, "$(run_name).bson"), result, safe = true);
@info "Results were saved into file $(savedir) --- $(run_name) (.bson / .jls)"
et = floor(time()-start)
@info "Elapsed time: $(et) s"
println("Results were saved into file $(savedir) --- $(run_name) (.bson / .jls)")

close(lg)
